# Nipoppy: Parkinson's Progression Markers Initiative dataset

This repository contains code to process tabular and imaging data from the Parkinson's Progression Markers Initiative (PPMI) dataset using the [Nipoppy framework](https://nipoppy.readthedocs.io/en/stable/).

Unless otherwise specified, instructions assume the current working directory is the Nipoppy root directory.

## Python environment

The root directory of the data contains a `uv` virtual environment in the `.venv` directory.
`uv` can be installed by following instructions [here](https://docs.astral.sh/uv/getting-started/installation/#installation-methods).

To activate the environment:
```bash
source .venv/bin/activate
```

## PPMI CSV files to download from LONI

**Note**
- The `global_config.json`'s `"CUSTOM"` field points to these files. They should be downloaded into `sourcedata/tabular`.
- Files downloaded from LONI have a timestamp for the date. Make sure the timestamps are correct in the global config file's `"SUBSTITUTIONS"` field.
    - The `idaSearch_<TIMESTAMP>.csv` file must also be symlinked to `idaSearch.csv` for the BIDS conversion to work.

Image collections
- `idaSearch.csv`
    - Advanced Search
    - Check every box in "Display in result" column
    - Check "DTI" + "MRI" + "fMRI" in "Modality"

Study data
- Study Docs: Data & Databases
    - `Code_List_-__Annotated_.csv`
    - `Data_Dictionary_-__Annotated_.csv`
- Subject Characteristics: Patient Status
    - `Participant_Status.csv`
- Subject Characteristics: Subject Demographics
    - `Age_at_visit.csv`
    - `Demographics.csv`
    - `Socio-Economics.csv`
- Medical History: Medical
    - `Clinical_Diagnosis.csv`
    - `Primary_Clinical_Diagnosis.csv`
- Motor Assessments: Motor / MDS-UPDRS
    - `MDS-UPDRS_Part_I.csv`
    - `MDS-UPDRS_Part_III.csv`
    - `MDS_UPDRS_Part_II__Patient_Questionnaire.csv`
    - `MDS-UPDRS_Part_I_Patient_Questionnaire.csv`
    - `MDS-UPDRS_Part_IV__Motor_Complications.csv`
- Non-motor Assessments: ALL
    - `Benton_Judgement_of_Line_Orientation.csv`
    - `Clock_Drawing.csv`
    - `Letter_-_Number_Sequencing.csv`
    - `Modified_Boston_Naming_Test.csv`
    - `Modified_Semantic_Fluency.csv`
    - `Montreal_Cognitive_Assessment__MoCA_.csv`
    - `Symbol_Digit_Modalities_Test.csv`
    - `University_of_Pennsylvania_Smell_Identification_Test_UPSIT.csv`

## Manifest generation

Requires `idaSearch.csv` as well as demographics/assessments files listed above.

Run:
```bash
./code/scripts/curation/generate_manifest.py --dataset . --regenerate
```

This creates the manifest file and the DICOM directory mapping. 

## DICOM download

### Get mapping of BIDS datatype type to PPMI MRI protocol names

Requires `idaSearch.csv`.

Run:
```bash
./code/scripts/curation/filter_image_descriptions.py --dataset . --overwrite
```

This will update two files (which are both tracked by Git):
- `code/imaging_descriptions/ppmi_imaging_descriptions.json`: mapping of BIDS datatype (and suffix) to protocol names
- `code/imaging_descriptions/ppmi_imaging_ignored.csv`: protocol names that will not be included in the dataset

These two files should be checked to ensure that any modification makes sense. If not, see log messages from `filter_image_descriptions.py` for instructions on how to update the filters in `code/nipoppy_ppmi/imaging_filters.py`.

### Get lists of LONI image IDs to download

Requires:
- `idaSearch.csv`
- `code/imaging_descriptions/ppmi_imaging_descriptions.json` generated by previous step
- Updated `<NIPOPPY_ROOT>/sourcedata/imaging/doughnut.tsv`

Run:
```bash
# regenerate the doughnut file if needed
nipoppy track-curation --dataset . --regenerate

# change --session-id and --chunk-size as needed
./code/scripts/dicom_reorg/fetch_dicom_downloads.py --dataset . --session-id BL --chunk-size 1000
```

### Download images from LONI

Follow instructions in log of previous step for creating a collection.

If needed, use utility script `code/scripts/dicom_reorg/download_from_loni.sh` to download directly to Compute Canada from the computer that initiated the LONI download (i.e. laptop)
- Need access to Compute Canada robot node for non-2FA SSH connections, see [here](https://docs.alliancecan.ca/wiki/Automation_in_the_context_of_multifactor_authentication).
- After initiating a download from LONI, download the CSV file with the download URLs and pass this to `download_from_loni.sh`

Example command:
```bash
# this script should be copied to and run from local computer
# make sure to update the session ID as appropriate
./download_from_loni.sh <PATH_TO_CSV_FILE> <USERNAME>@robot.rorqual.alliancecan.ca <NIPOPPY_ROOT>/sourcedata/imaging/downloads/ses-BL
```

After the downloads are complete, rename the files to something like `240924_ses1_list12_dataset-{INDEX}.zip` if needed (will make unzipping easier).

### Unzip downloaded images

Can use utility job script `code/scripts/dicom_reorg/unzip_loni_downloads.sh`. Example job submission command:
```bash
sbatch --array=1-2 --account=rrg-jbpoline --output=./logs/hpc/%x_%A_%a.out ./code/scripts/dicom_reorg/unzip_loni_downloads.sh ./sourcedata/imaging/downloads/ses-BL/260107-BL-list1_dataset.zip ./sourcedata/imaging/pre_reorg/ses-BL
```

Move subdirectories out of parent `PPMI` directory and delete `PPMI` directory.

Then regenerate the curation status file:
```bash
nipoppy track-curation --dataset . --regenerate
```

## DICOM reorg

Using custom script. This step is quite slow so need to do it in a Slurm job (update: on Rorqual it is much faster). Sample submission command:
```bash
sbatch --account=rrg-jbpoline --time=1:00:00 --mem=1G --job-name=ppmi_reorg --output="./logs/hpc/%x_%j.out" --wrap="source /lustre09/project/6061841/shared/datasets/ppmi/.venv/bin/activate && ./code/scripts/dicom_reorg/dicom_reorg.py --dataset ."
``` 

**Note**: may need to `nipoppy track-curation --regenerate` again (? to check)

## Create SquashFS archive

Sample commands:
```bash
DATASET_ROOT=`realpath .`; DPATH_SQUASH=$DATASET_ROOT/sourcedata/imaging/squash; FPATH_SQUASH=$DPATH_SQUASH/ses-BL/260107-BL-list1.squashfs; DPATH_LOGS=$DATASET_ROOT/logs/hpc; DPATH_CODE=$DATASET_ROOT/code/scripts/dicom_reorg; sbatch --account=rrg-jbpoline --output=$DPATH_LOGS/%x-%j.out $DPATH_CODE/make_squash.sh --no-chmod --exclude $DPATH_CODE/exclude.txt --move /ppmi/sourcedata/imaging $FPATH_SQUASH $DATASET_ROOT/sourcedata/imaging/pre_reorg $DATASET_ROOT/sourcedata/imaging/post_reorg
```

## BIDS conversion

### Run Heudiconv stage 1

Sample command:
```bash
nipoppy bidsify --pipeline heudiconv --pipeline-step prepare --session-id BL --hpc slurm
```

### Heudiconv testing

Sample command:
```bash
rm -rf ./code/nipoppy_ppmi/fake_bids
./code/nipoppy_ppmi/heuristic.py --dataset . --session-id BL
```

Make sure the above script does not produce errors.

Rename `.heudiconv` directory in `<NIPOPPY_ROOT>/bids`

### Run Heudiconv stage 2

Sample command:
```bash
nipoppy bidsify --pipeline heudiconv --pipeline-step convert --session-id BL --hpc slurm
```

### Clean up `.heudiconv` directories

- rename
- tar (inside `bids/` directory: `tar -czvf .heudiconv-<EXTRA>.tar.gz .heudiconv-<EXTRA>/`)
- delete (IMPORTANT: only do this if SquashFS creation is done!)

### Update doughnut with BIDS data

```bash
nipoppy track-curation . --regenerate
```

### Fix DWI data

```bash
./code/scripts/curation/add_bval_bvec_to_B0_dwi.py --dataset . --session-id BL
```

### Delete pre-reorg and post-reorg DICOM files

Only if SquashFS file has been created!

Sample commands:
```bash
rm -rf ./sourcedata/imaging/pre_reorg/ses-*
rm -rf ./sourcedata/imaging/post_reorg/sub-*
```

## Other (older) notes

### PPMI data portal (LONI IDA)

* Some search fields in LONI search tool cannot be trusted
    * Examples:
        * `Modality`
            * `Modality=DTI` can have anatomical images, and there are diffusion images with `MRI` modality
        * `Weighting` (under `Imaging Protocol`)
            * Some T1s have `Weighting=PD`
    * We classify image modalities/contrast only based on the `Image Description` column
        * This can also lead to issues, for example when a subject has the same description string for all of their scans. In that case, we manually determine the image modality/contrast and hard-code the mapping in `heuristic.py` for HeuDiConv
* LONI viewer sometimes shows seemingly bad/corrupted files but they are actually fine once we convert them
    * Observed for some diffusion images (tend to have ~2700 slices according to the LONI image viewer)

### Compute Canada

* Some subjects have a huge amount of small DICOM files, which causes us to exceed the inode quota on `/scratch`
    * We opted to create SquashFS archives/filesystems, which count as 1 inode and can be mounted as a filesystem in Singularity container (using the `--overlay` argument). This is similar to [how McGill/NeuroHub stores UK Biobank data on Compute Canada](https://arxiv.org/abs/2002.06129)

## BIDS

### BIDS data file naming

<!-- TODO: update link/path once tabular is moved under workflow -->
The [tabular/ppmi_imaging_descriptions.json](https://github.com/neurodatascience/nipoppy-ppmi/blob/main/nipoppy/workflow/tabular/ppmi_imaging_descriptions.json) file is used to determine the BIDS datatype and suffix (contrast) associated with an image's MRI series description. It will be updated as new data is processed.

Here is a description of the available BIDS data and the tags that can appear in their filenames:

- `anat`
  - The available suffixes are: `T1w`, `T2w`, `T2starw`, and `FLAIR`
  - Most images have an `acq` tag:
    - Non-neuromelanin images: `acq-<plane><type>`, where
        - `<plane>` is one of: `sag`, `ax`, or `cor` (for sagittal, axial, or coronal scans respectively)
        - `<type>` is one of: `2D`, or `3D`
    - Neuromelanin images: `acq-NM`
  - For some images, the acquisition plane (`sag`/`ax`/`cor`) or type (`2D`/`3D`) cannot be easily obtained. In those cases, the filename will not contain an `acq` tag.
- `dwi`
  - All imaging files have the `dwi` suffix.
  - Most images have a `dir` tag corresponding to the phase-encoding direction. This is one of: `LR`, `RL`, `AP`, or `PA`
  - Images where the phase-encoding direction cannot be easily inferred from the series description string do not have a `dir` tag.
  - Some participants have multi-shell sequences for their diffusion data. These files will have an additional `acq-B<value>` tag, where `value` is the b-value for that sequence.

Currently, only structural (`anat`) and diffusion (`dwi`) MRI data are supported. Functional (`func`) data has not been converted to the BIDS format yet.

### HeuDiConv errors

#### Not solved yet

* `AttributeError: 'Dataset' object has no attribute 'StackID'`
    * [Vincent previously had the same issue](https://github.com/nipy/heudiconv/issues/517), unclear if/how it was fixed. Error could be because the images are in a single big DICOM instead of many small DICOM files
* `AssertionError: Conflicting study identifiers found`
    * Could be because all of a subject's DICOMs are pooled together in the `dicom_org` step, in which case this can be fixed by manually running HeuDiConv for each image
* `numpy.AxisError: axis 1 is out of bounds for array of dimension 1`
    * Only happened for one image so far
    * See https://github.com/nipy/heudiconv/issues/670 and https://github.com/nipy/nibabel/issues/1245
* `AssertionError (assert HEUDICONV_VERSION_JSON_KEY not in json_)`
    * Thrown by HeuDiConv
* `AssertionError: we do expect some files since it was called (assert bids_files, "we do expect some files since it was called")`
    * Thrown by HeuDiConv

### Notes on `dwi` data

* Some subjects only have a single diffusion image (e.g., `Ax DTI`), might not be usable
* Some subjects have 2 diffusion images, but they have the same description string (e.g., `DTI_gated`)
    * Checked some cases after BIDS conversion, and the JSON sidecars seem to have the same `PhaseEncodingDirection` (`j-`)
* Some subjects have multi-shell sequences. Their files seem to follow the following pattern:
    * `dir-PA`: 1 `B0`, 1 `B700`, 1 `B1000`, and 1 `B2000` image
    * `dir-AP`: 4 `B0` images
* Some (~2 for `ses-BL`) subjects have `dir-AP` for all their diffusion images
    * Seem to have 4 `dir-AP` `B0` images and 4 other `dir-AP` images (according to their description string)
* Some diffusion images do not contain raw data, but rather tensor model results (`FA`, `ADC`, `TRACEW`). Some of these have been excluded before BIDS conversion, but not all of them
